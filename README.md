# Exploring â€” Tokenizers & Data Loaders ğŸš€

When I started my NLP journey, I was fascinated by how **raw text** magically turns into **numbers that models understand**. So in this notebook, I decided to explore and experiment with **different tokenization libraries** ğŸ§° and **data loading techniques** ğŸ“¦ to get a strong practical foundation.

This repository documents that learning journey â€” from basic tokenization to efficient batching with data loaders!

---

## âœ¨ What I Explored

### ğŸ”¤ **Tokenization Libraries & Techniques**

I experimented with several popular NLP libraries to understand how they break down text:

- ğŸ“ **NLTK** â€” classic word tokenization, n-grams & frequency distributions  
- ğŸ§¬ **spaCy** â€” powerful multilingual tokenization (English & French models)  
- ğŸ¤— **Hugging Face Transformers** â€”  
  - `BertTokenizer` â†’ based on the **WordPiece algorithm**, which breaks words into subword units to handle rare or unknown tokens efficiently.  
  - `XLNetTokenizer` â†’ uses **SentencePiece** with a **Unigram language model**, enabling language-independent, unsupervised subword tokenization.  
- ğŸ”¡ **SentencePiece** â€” standalone subword tokenization library used internally by several modern NLP models.  
- ğŸ§  **TorchText** â€” `basic_english` tokenizer and vocabulary building utilities for quick text processing.

This hands-on exploration helped me see **how different tokenizers segment text**, **what kind of tokens they produce**, and how **tokenization choices affect downstream NLP tasks**.

---

### ğŸ§° **Data Loading & Batching**

Alongside tokenization, I explored how to **feed data efficiently to models** using **PyTorchâ€™s DataLoader**:

- ğŸ“¦ **Batching** â€” loading data in mini-batches to speed up training  
- ğŸ”„ **Shuffling** â€” ensuring models donâ€™t overfit to sequence order  
- ğŸ§  **Memory Management** â€” preventing memory overflow during training with efficient loaders

---

## ğŸ“š Why This Matters

Tokenization and data loading are **the first steps of every NLP pipeline**. Mastering them early helps you:

- Understand how text becomes model input ğŸ§   
- Improve performance and training efficiency âš¡  
- Build solid foundations for future NLP projects ğŸš€

---

