{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n",
      "Collecting transformers==4.42.1\n",
      "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (0.35.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.1)\n",
      "  Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from transformers==4.42.1) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.42.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->transformers==4.42.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->transformers==4.42.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->transformers==4.42.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->transformers==4.42.1) (2025.8.3)\n",
      "Using cached transformers-4.42.1-py3-none-any.whl (9.3 MB)\n",
      "Using cached tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.0\n",
      "    Uninstalling transformers-4.57.0:\n",
      "      Successfully uninstalled transformers-4.57.0\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.42.1\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.4/13.9 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.5/13.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.3/13.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.4/13.9 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.0/13.9 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.8 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.8 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.4/6.3 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.3 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 numpy-2.3.3 preshed-3.0.10 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
      "torchtext 0.17.2 requires torch==2.2.2, but you have torch 2.8.0 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.3.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ------ -------------------------------- 2.1/12.8 MB 857.5 kB/s eta 0:00:13\n",
      "     ------ -------------------------------- 2.1/12.8 MB 857.5 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.4/12.8 MB 838.9 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 853.3 kB/s eta 0:00:12\n",
      "     --------- ----------------------------- 3.1/12.8 MB 941.7 kB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.0 MB/s eta 0:00:09\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 1.4 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.6 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 2.1/16.3 MB 13.0 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 4.5/16.3 MB 12.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 7.1/16.3 MB 11.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 9.4/16.3 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.5/16.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 12.6/16.3 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 15.2/16.3 MB 10.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 16.3/16.3 MB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.6 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.7/12.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 9.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "Successfully installed numpy-2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "torchtext 0.17.2 requires torch==2.2.2, but you have torch 2.8.0 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.2\n",
      "  Using cached torch-2.2.2-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Using cached torch-2.2.2-cp312-cp312-win_amd64.whl (198.5 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0\n",
      "    Uninstalling torch-2.8.0:\n",
      "      Successfully uninstalled torch-2.8.0\n",
      "Successfully installed torch-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n",
      "torchvision 0.23.0 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.17.2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torchtext==0.17.2) (2.2.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchtext==0.17.2) (2024.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from requests->torchtext==0.17.2) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.17.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rajveer gupta\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
      "Collecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.0-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
      "torchvision 0.23.0 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rajveer\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Rajveer\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch  Basic  English Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens [['hello', ',', 'i', 'am', 'rajveer', 'gupta'], ['exploring', 'different', 'tokenizers'], ['learning', 'ml', ',', 'dl', ',', 'gen', 'ai', 'is', 'wonderful'], ['my', 'learning', 'in', 'college', 'is', 'going', 'crazy'], ['extremely', 'grateful', 'for', 'getting', 'the', 'oppotunity', 'to', 'learn', 'new', 'things'], ['want', 'to', 'keep', 'exploring', 'new', 'things', 'just', 'like', 'this']]\n",
      "indices\n",
      "[[20, 0, 21, 8, 30, 19], [1, 11, 33], [3, 27, 0, 12, 0, 15, 7, 2, 35], [28, 3, 22, 9, 2, 17, 10], [13, 18, 14, 16, 31, 29, 6, 25, 4, 5], [34, 6, 24, 1, 4, 5, 23, 26, 32]]\n"
     ]
    }
   ],
   "source": [
    "input_text=[\"Hello,I am Rajveer Gupta\",\n",
    "            \"Exploring different tokenizers\",\n",
    "           \"Learning ML,DL , Gen AI is Wonderful\",\n",
    "           \"My Learning in college is going crazy\",\n",
    "           \"Extremely Grateful for getting the oppotunity to learn new things\",\n",
    "           \"Want to keep exploring new things just like this \"]\n",
    "tokenizer=get_tokenizer(\"basic_english\")\n",
    "vocab=build_vocab_from_iterator(map(tokenizer,input_text))\n",
    "def tokens_yield(input_text):\n",
    "    for sentence in input_text:\n",
    "        yield tokenizer(sentence)\n",
    "def print_tokens_and_their_indices(input_text):\n",
    "    tokens=list(tokens_yield(input_text))\n",
    "    print(\"Tokens\",tokens)\n",
    "    print(\"indices\")\n",
    "    token_indices=[vocab(token) for token in tokens]\n",
    "    print(token_indices)\n",
    "print_tokens_and_their_indices(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Rajveer', 'Gupta']\n",
      "['Exploring', 'different', 'tokenizers']\n",
      "['Learning', 'ML', ',', 'DL', ',', 'Gen', 'AI', 'is', 'Wonderful']\n",
      "['My', 'Learning', 'in', 'college', 'is', 'going', 'crazy']\n",
      "['Extremely', 'Grateful', 'for', 'getting', 'the', 'oppotunity', 'to', 'learn', 'new', 'things']\n",
      "['Want', 'to', 'keep', 'exploring', 'new', 'things', 'just', 'like', 'this']\n"
     ]
    }
   ],
   "source": [
    "input_text=[\"Hello,I am Rajveer Gupta\",\n",
    "            \"Exploring different tokenizers\",\n",
    "           \"Learning ML,DL , Gen AI is Wonderful\",\n",
    "           \"My Learning in college is going crazy\",\n",
    "           \"Extremely Grateful for getting the oppotunity to learn new things\",\n",
    "           \"Want to keep exploring new things just like this \"]\n",
    "def tokens_yield(input_text):\n",
    "    for sentence in input_text:\n",
    "        yield word_tokenize(sentence)\n",
    "tokens=tokens_yield(input_text)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy English Tokenizer Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens [['Hello', ',', 'I', 'am', 'Rajveer', 'Gupta'], ['Exploring', 'different', 'tokenizers'], ['Learning', 'ML', ',', 'DL', ',', 'Gen', 'AI', 'is', 'Wonderful'], ['My', 'Learning', 'in', 'college', 'is', 'going', 'crazy'], ['Extremely', 'Grateful', 'for', 'getting', 'the', 'oppotunity', 'to', 'learn', 'new', 'things'], ['Want', 'to', 'keep', 'exploring', 'new', 'things', 'just', 'like', 'this']]\n",
      "indices\n",
      "[[13, 0, 14, 20, 17, 12], [8, 23, 36], [1, 15, 0, 7, 0, 10, 6, 2, 19], [16, 1, 28, 21, 2, 27, 22], [9, 11, 25, 26, 34, 33, 5, 31, 3, 4], [18, 5, 30, 24, 3, 4, 29, 32, 35]]\n"
     ]
    }
   ],
   "source": [
    "input_text=[\"Hello,I am Rajveer Gupta\",\n",
    "            \"Exploring different tokenizers\",\n",
    "           \"Learning ML,DL , Gen AI is Wonderful\",\n",
    "           \"My Learning in college is going crazy\",\n",
    "           \"Extremely Grateful for getting the oppotunity to learn new things\",\n",
    "           \"Want to keep exploring new things just like this \"]\n",
    "tokenizer=get_tokenizer(\"spacy\",language=\"en_core_web_sm\")\n",
    "vocab=build_vocab_from_iterator(map(tokenizer,input_text))\n",
    "def tokens_yield(input_text):\n",
    "    for sentence in input_text:\n",
    "        yield tokenizer(sentence)\n",
    "def print_tokens_and_their_indices(input_text):\n",
    "    tokens=list(tokens_yield(input_text))\n",
    "    print(\"Tokens\",tokens)\n",
    "    print(\"indices\")\n",
    "    token_indices=[vocab(token) for token in tokens]\n",
    "    print(token_indices)\n",
    "print_tokens_and_their_indices(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Based Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Tokenizer Working on Word Piece Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens [['hello', ',', 'i', 'am', 'raj', '##ve', '##er', 'gupta'], ['exploring', 'different', 'token', '##izer', '##s'], ['learning', 'ml', ',', 'dl', ',', 'gen', 'ai', 'is', 'wonderful'], ['my', 'learning', 'in', 'college', 'is', 'going', 'crazy'], ['extremely', 'grateful', 'for', 'getting', 'the', 'opportunity', 'to', 'learn', 'new', 'things'], ['want', 'to', 'keep', 'exploring', 'new', 'things', 'just', 'like', 'this']]\n",
      "indices\n",
      "[[24, 0, 25, 12, 34, 10, 7, 23], [1, 15, 37, 8, 9], [3, 31, 0, 16, 0, 19, 11, 2, 39], [32, 3, 26, 13, 2, 21, 14], [17, 22, 18, 20, 35, 33, 6, 29, 4, 5], [38, 6, 28, 1, 4, 5, 27, 30, 36]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "input_text = [\n",
    "    \"Hello, I am Rajveer Gupta\",\n",
    "    \"Exploring different tokenizers\",\n",
    "    \"Learning ML, DL, Gen AI is Wonderful\",\n",
    "    \"My Learning in college is going crazy\",\n",
    "    \"Extremely Grateful for getting the opportunity to learn new things\",\n",
    "    \"Want to keep exploring new things just like this\"\n",
    "]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokens_yield(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer.tokenize(sentence)\n",
    "vocab = build_vocab_from_iterator(tokens_yield(input_text))\n",
    "def print_tokens_and_their_indices(input_text):\n",
    "    tokens=list(tokens_yield(input_text))\n",
    "    print(\"Tokens\",tokens)\n",
    "    print(\"indices\")\n",
    "    token_indices=[vocab(token) for token in tokens]\n",
    "    print(token_indices)\n",
    "print_tokens_and_their_indices(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Piece and Unigram Algorithms from XLNet Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a671fed9b44d21be0f222b230147aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a9906d2aa94096b25af1025b60ed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4863ced09e9b42d19bd7307beb17112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens [['▁', 'Hello', ',', '▁I', '▁am', '▁Raj', 've', 'er', '▁Gupta'], ['▁Exp', 'lor', 'ing', '▁different', '▁token', 'izer', 's'], ['▁Learning', '▁', 'ML', ',', '▁', 'DL', ',', '▁Gen', '▁A', 'I', '▁is', '▁', 'Wonderful'], ['▁My', '▁Learning', '▁in', '▁college', '▁is', '▁going', '▁crazy'], ['▁Extreme', 'ly', '▁G', 'rate', 'ful', '▁for', '▁getting', '▁the', '▁opportunity', '▁to', '▁learn', '▁new', '▁things'], ['▁Want', '▁to', '▁keep', '▁exploring', '▁new', '▁things', '▁just', '▁like', '▁this']]\n",
      "indices\n",
      "[[0, 8, 1, 27, 31, 29, 20, 12, 26], [22, 16, 14, 34, 47, 15, 19], [2, 0, 10, 1, 0, 7, 1, 25, 21, 9, 3, 0, 11], [28, 2, 39, 32, 3, 38, 33], [23, 17, 24, 18, 13, 36, 37, 45, 44, 6, 42, 4, 5], [30, 6, 41, 35, 4, 5, 40, 43, 46]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "input_text = [\n",
    "    \"Hello, I am Rajveer Gupta\",\n",
    "    \"Exploring different tokenizers\",\n",
    "    \"Learning ML, DL, Gen AI is Wonderful\",\n",
    "    \"My Learning in college is going crazy\",\n",
    "    \"Extremely Grateful for getting the opportunity to learn new things\",\n",
    "    \"Want to keep exploring new things just like this\"\n",
    "]\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "def tokens_yield(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer.tokenize(sentence)\n",
    "vocab = build_vocab_from_iterator(tokens_yield(input_text))\n",
    "def print_tokens_and_their_indices(input_text):\n",
    "    tokens=list(tokens_yield(input_text))\n",
    "    print(\"Tokens\",tokens)\n",
    "    print(\"indices\")\n",
    "    token_indices=[vocab(token) for token in tokens]\n",
    "    print(token_indices)\n",
    "print_tokens_and_their_indices(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Grateful for getting the opportunity to learn new things', 'Learning ML, DL, Gen AI is Wonderful']\n",
      "['Want to keep exploring new things just like this', 'My Learning in college is going crazy']\n",
      "['Hello, I am Rajveer Gupta', 'Exploring different tokenizers']\n"
     ]
    }
   ],
   "source": [
    "input_text = [\n",
    "    \"Hello, I am Rajveer Gupta\",\n",
    "    \"Exploring different tokenizers\",\n",
    "    \"Learning ML, DL, Gen AI is Wonderful\",\n",
    "    \"My Learning in college is going crazy\",\n",
    "    \"Extremely Grateful for getting the opportunity to learn new things\",\n",
    "    \"Want to keep exploring new things just like this\"\n",
    "]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,sentences):\n",
    "        self.sentences=sentences\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.sentences[idx]\n",
    "custom_dataset=CustomDataset(input_text)\n",
    "dataloader=DataLoader(custom_dataset,batch_size=2,shuffle=True)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader On Basic English tokenizer of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset Length: 6\n",
      "Sample Items:\n",
      "Item 1: tensor([20,  0, 21,  8, 30, 19])\n",
      "Item 2: tensor([ 1, 11, 33])\n",
      "Item 3: tensor([ 3, 27,  0, 12,  0, 15,  7,  2, 35])\n",
      "Item 4: tensor([28,  3, 22,  9,  2, 17, 10])\n",
      "Item 5: tensor([13, 18, 14, 16, 31, 29,  6, 25,  4,  5])\n",
      "Item 6: tensor([34,  6, 24,  1,  4,  5, 23, 26, 32])\n"
     ]
    }
   ],
   "source": [
    "input_text = [\n",
    "    \"Hello, I am Rajveer Gupta\",\n",
    "    \"Exploring different tokenizers\",\n",
    "    \"Learning ML, DL, Gen AI is Wonderful\",\n",
    "    \"My Learning in college is going crazy\",\n",
    "    \"Extremely Grateful for getting the opportunity to learn new things\",\n",
    "    \"Want to keep exploring new things just like this\"\n",
    "]\n",
    "# Define a custom data set\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.sentences[idx])\n",
    "        # Convert tokens to tensor indices using vocab\n",
    "        tensor_indices = [self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(tensor_indices)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab=build_vocab_from_iterator(map(tokenizer,input_text))#both map and yield function are same \n",
    "# Create an instance of your custom data set\n",
    "custom_dataset = CustomDataset(input_text, tokenizer, vocab)\n",
    "\n",
    "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
    "print(\"Sample Items:\")\n",
    "for i in range(6):\n",
    "    sample_item = custom_dataset[i]\n",
    "    print(f\"Item {i + 1}: {sample_item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally Applying DataLoader And Spacy French tokenizer on a French Input Data To Test My Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tensors\n",
    "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,  63,  40,  13,  89,  67,  13,  79,   0],\n",
      "        [ 30,  18,  19,  88,  21,   2,   0,   0,   0],\n",
      "        [  1,   3,  98,   5, 116,  99,  66,   2,   0]])\n",
      "tensor([[  1,   3,  70,  46,  10,  81,  78,   5,  21,   0],\n",
      "        [ 33,  71, 122, 117,  52,   2,   0,   0,   0,   0],\n",
      "        [ 35,   8,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 12, 119,  39,   8,   5,  84,  59,  54, 115,   0]])\n",
      "tensor([[  1, 105,  41,   0,   0,   0,   0,   0],\n",
      "        [ 11,   4,  74,   2,   0,   0,   0,   0],\n",
      "        [120,  97,  75,   4,   6,  93,  20,   7],\n",
      "        [ 31,  43,   8,  15,  57,  73,   0,   0]])\n",
      "tensor([[ 29,  24,  96, 109,  48,  61,  94,  18,   6, 118,  23,  65,   7],\n",
      "        [  1,   3,  14, 100,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 32,  85,  42,  80,  87,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [ 25, 101,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "tensor([[  1,   3,  64,  22,  77,  16,  91,  17, 114, 121,  15, 102,   0],\n",
      "        [ 12,  69,  51,  49,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  1, 113,  55,   6,  86,  53,  47,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   3,  82,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
      "tensor([[ 26,  45,   2,   0,   0,   0,   0,   0],\n",
      "        [  1,  16, 103,  17,   0,   0,   0,   0],\n",
      "        [ 37,   4,  19,  92,  95,   7,   0,   0],\n",
      "        [ 11,   4, 111,  50,  68,   5,   9,   0]])\n",
      "tensor([[  1,   3,  76,   0,   0,   0,   0,   0,   0],\n",
      "        [  1,   3,  14,  20,  58,  44,   6,  72,   0],\n",
      "        [ 28,   4,  10,   9,   0,   0,   0,   0,   0],\n",
      "        [ 38,  10, 107,   9,   0,   0,   0,   0,   0]])\n",
      "tensor([[ 34, 112, 104, 106, 108,  56,   0],\n",
      "        [ 36,  62,  90, 110,  60,  83,   0]])\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Ceci est une phrase.\",\n",
    "    \"C'est un autre exemple de phrase.\",\n",
    "    \"Voici une troisième phrase.\",\n",
    "    \"Il fait beau aujourd'hui.\",\n",
    "    \"J'aime beaucoup la cuisine française.\",\n",
    "    \"Quel est ton plat préféré ?\",\n",
    "    \"Je t'adore.\",\n",
    "    \"Bon appétit !\",\n",
    "    \"Je suis en train d'apprendre le français.\",\n",
    "    \"Nous devons partir tôt demain matin.\",\n",
    "    \"Je suis heureux.\",\n",
    "    \"Le film était vraiment captivant !\",\n",
    "    \"Je suis là.\",\n",
    "    \"Je ne sais pas.\",\n",
    "    \"Je suis fatigué après une longue journée de travail.\",\n",
    "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
    "    \"Je vais chez le médecin cet après-midi.\",\n",
    "    \"La musique adoucit les mœurs.\",\n",
    "    \"Je dois acheter du pain et du lait.\",\n",
    "    \"Il y a beaucoup de monde dans cette ville.\",\n",
    "    \"Merci beaucoup !\",\n",
    "    \"Au revoir !\",\n",
    "    \"Je suis ravi de vous rencontrer enfin !\",\n",
    "    \"Les vacances sont toujours trop courtes.\",\n",
    "    \"Je suis en retard.\",\n",
    "    \"Félicitations pour ton nouveau travail !\",\n",
    "    \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
    "    \"À quelle heure est le prochain train ?\",\n",
    "    \"Bonjour !\",\n",
    "    \"C'est génial !\"\n",
    "]\n",
    "\n",
    "\n",
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self,sentences,vocab,tokenizer):\n",
    "        self.sentences=sentences\n",
    "        self.vocab=vocab\n",
    "        self.tokenizer=tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self,idx):\n",
    "        tokens=self.tokenizer(self.sentences[idx])\n",
    "        token_indices=[self.vocab[token] for token in tokens]\n",
    "        return torch.tensor(token_indices)\n",
    "tokenizer=get_tokenizer('spacy',language='fr_core_news_sm')\n",
    "vocab=build_vocab_from_iterator(map(tokenizer,corpus))\n",
    "custom_dataset=CustomDataset1(sentences=corpus,vocab=vocab,tokenizer=tokenizer)\n",
    "dataloader=DataLoader(dataset=custom_dataset,batch_size=4,shuffle=True,collate_fn=collate_fn)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "prev_pub_hash": "b07eeb64da639d921152399e4e12fe8c69fe4147591cddf616f678d21334129b"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
